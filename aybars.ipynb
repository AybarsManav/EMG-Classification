{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from data_preprocess import data_preprocess\n",
    "from feature_extractor import FeatureExtractor\n",
    "from test_model import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and extract gesture segments from the data.\n",
    "data_path = 'Project_Data_EE4C12_S&S_EMG.csv'\n",
    "gesture_windows = data_preprocess(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each gesture\n",
    "feature_extractor = FeatureExtractor()\n",
    "features = feature_extractor.extract_features(gesture_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the class balances using histogram of class samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balances = features['gesture'].value_counts()\n",
    "print(class_balances)\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_balances.plot(kind='bar', color='skyblue')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that there are too few samples (8 samples) from class 7. We can give that class a higher weight in the loss functions of the future models but even training might not be good enough with this many samples. So we decided to remove the class 7 from the data.\n",
    "\n",
    "Also, unmarked data belonging to class 0 dominates other classes in terms of number of samples and generally classifiers work better when all classes are balanced. So we can give it a lower weight or subsample the class 0. We chose to subsample the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove class 7 from the features dataframe\n",
    "features_filtered = features[features['gesture'] != 7]\n",
    "\n",
    "# Get the number of samples for the smallest class (excluding class 0)\n",
    "min_class_count = features_filtered['gesture'].value_counts().min()\n",
    "\n",
    "# Subsample class 0 to have the same number of samples as the smallest class\n",
    "class_0_indices = features_filtered[features_filtered['gesture'] == 0].index\n",
    "subsampled_class_0_indices = np.random.choice(class_0_indices, min_class_count, replace=False)\n",
    "\n",
    "# Get the indices of all other classes\n",
    "other_class_indices = features_filtered[features_filtered['gesture'] != 0].index\n",
    "\n",
    "# Combine the subsampled class 0 indices with the other class indices\n",
    "new_indices = np.concatenate([subsampled_class_0_indices, other_class_indices])\n",
    "\n",
    "# Create the new features dataframe\n",
    "features_balanced = features_filtered.loc[new_indices]\n",
    "\n",
    "# Display the class distribution of the new features dataframe\n",
    "plt.figure(figsize=(10, 6))\n",
    "features_balanced['gesture'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Balanced Class Distrubution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_balanced.drop(columns=['gesture']), features_balanced['gesture'], test_size=0.15, random_state=42)\n",
    "\n",
    "# Reset indices of the resulting datasets\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create train2-validation split\n",
    "X_train2, X_validation, y_train2, y_validation = train_test_split(X_train, y_train, test_size=0.30, random_state=42)\n",
    "\n",
    "X_train2.reset_index(drop=True, inplace=True)\n",
    "y_train2.reset_index(drop=True, inplace=True)\n",
    "X_validation.reset_index(drop=True, inplace=True)\n",
    "y_validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_train shape: {X_train2.shape}\")\n",
    "print(f\"y_train shape: {y_train2.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"X_validation shape: {X_validation.shape}\")\n",
    "print(f\"y_validation shape: {y_validation.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform other splits\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train2_scaled = scaler.transform(X_train2)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# Convert the scaled data back to DataFrame for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=X_train2.columns)\n",
    "X_validation_scaled = pd.DataFrame(X_validation_scaled, columns=X_validation.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best C value for the Logistic Regression model based on accuracy on validation set. Then train the best model using whole train set and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "max_min_f1_score = 0\n",
    "best_c_value = None\n",
    "\n",
    "for c_val in C_values:\n",
    "    logistic_model = LogisticRegression(C=c_val, max_iter=1000, multi_class=\"multinomial\",\n",
    "                                        tol=1e-4, random_state=42)\n",
    "\n",
    "    # Train the model using the scaled training data (not containing validation set samples)\n",
    "    logistic_model.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "    # Print the training accuracy\n",
    "    train_accuracy = logistic_model.score(X_train2_scaled, y_train2)\n",
    "\n",
    "    # Validate model using the scaled validation data\n",
    "    metrics = test_model(logistic_model, X_validation_scaled, y_validation)\n",
    "    print(f\"LR model with c={c_val} reached {metrics['min_f1_score']:.2f} minimum f1 score on the validation set.\")\n",
    "\n",
    "    if (max_min_f1_score < metrics['min_f1_score']):\n",
    "        max_min_f1_score = metrics['min_f1_score']\n",
    "        best_c_value = c_val\n",
    "\n",
    "# Train the best model with whole training set\n",
    "best_model = LogisticRegression(C=best_c_value, max_iter=1000, multi_class=\"multinomial\",\n",
    "                                tol=1e-4, random_state=42)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test the best model using the scaled test data\n",
    "metrics_lr = test_model(best_model, X_test_scaled, y_test, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune regularization parameter alpha of Ridge Classifier using the validation set. We chose the alpha value with highest accuracy on validation set. Then we trained that model using the whole training set and tested it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = [1e-5, 1e-4, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "max_min_f1_score = 0\n",
    "best_alpha_value = None\n",
    "for alpha_val in alpha_vals:\n",
    "    model = RidgeClassifier(alpha=alpha_val, max_iter=1000,\n",
    "                                        tol=1e-4, random_state=42)\n",
    "\n",
    "    # Train the model using the scaled training data (not containing validation set samples)\n",
    "    model.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "    # Print the training accuracy\n",
    "    train_accuracy = model.score(X_train2_scaled, y_train2)\n",
    "\n",
    "    # Validate model using the scaled validation data\n",
    "    metrics = test_model(model, X_validation_scaled, y_validation)\n",
    "    print(f\"Ridge Classifier model with alpha={alpha_val} reached {metrics['min_f1_score']:.2f} minimum f1 score on the validation set.\")\n",
    "\n",
    "    if (max_min_f1_score < metrics['min_f1_score']):\n",
    "        max_min_f1_score = metrics['min_f1_score']\n",
    "        best_alpha_value = alpha_val\n",
    "\n",
    "# Train the best model with whole training set\n",
    "highest_accuracy_model = RidgeClassifier(alpha=best_alpha_value, max_iter=1000,\n",
    "                                        tol=1e-4, random_state=42)\n",
    "highest_accuracy_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test the best model using the scaled test data\n",
    "metrics_ridge_classifier = test_model(highest_accuracy_model, X_test_scaled, y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters of an MLP are as follows:\n",
    "1. Number of hidden layers\n",
    "2. Hidden layer dimensions\n",
    "3. Activation functions\n",
    "4. LR scheduling\n",
    "\n",
    "We tuned LR scheduling using the built-in functionality of MLP using 10% of its training data as a validation set. However, we kept using our own validation set to tune the rest of the hyperparameters using a grid search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = [1, 2, 3, 4]\n",
    "hidden_layer_sizes = [(100,), (200,), (300,)]\n",
    "activation_functions = ['relu', 'tanh', 'logistic']\n",
    "highest_min_f1_score = 0\n",
    "highest_accuracy_model_params = {}\n",
    "\n",
    "for n_layers in n_hidden_layers:\n",
    "    for layer_size in hidden_layer_sizes:\n",
    "        for activation in activation_functions:\n",
    "            layer_size = (layer_size[0],) * n_layers\n",
    "            mlp_model = MLPClassifier(hidden_layer_sizes=layer_size, max_iter=1000, random_state=42,\n",
    "                                       activation=activation, learning_rate='adaptive',\n",
    "                                       learning_rate_init=1e-3, tol=1e-4)\n",
    "\n",
    "            # Train the model using the scaled training data\n",
    "            mlp_model.fit(X_train2_scaled, y_train2)\n",
    "            \n",
    "            # Validate model using the scaled validation data\n",
    "            metrics = test_model(mlp_model, X_validation_scaled, y_validation)\n",
    "            min_f1_score = metrics['min_f1_score']\n",
    "            print(f\"MLP model with {n_layers} hidden layers, {layer_size} neurons, and {activation} activation function reached {min_f1_score:.2f} minimum f1 score on the validation set.\")\n",
    "\n",
    "            if (highest_min_f1_score < min_f1_score):\n",
    "                highest_min_f1_score = min_f1_score\n",
    "                highest_accuracy_model_params['n_hidden_layers'] = n_layers\n",
    "                highest_accuracy_model_params['hidden_layer_sizes'] = layer_size\n",
    "                highest_accuracy_model_params['activation'] = activation         \n",
    "\n",
    "print(highest_accuracy_model_params)\n",
    "\n",
    "# Train the best model using the scaled training data\n",
    "best_model = MLPClassifier(hidden_layer_sizes=highest_accuracy_model_params['hidden_layer_sizes'],\n",
    "                            max_iter=1000, random_state=42, activation=highest_accuracy_model_params['activation'],\n",
    "                              learning_rate='adaptive', learning_rate_init=1e-3, tol=1e-4)\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the training accuracy\n",
    "train_accuracy_mlp = best_model.score(X_train_scaled, y_train)\n",
    "\n",
    "metrics_mlp = test_model(best_model, X_test_scaled, y_test, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_scaled is the data we are applying PCA on\n",
    "# Step 1: Apply PCA to the training data and fit the model\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X_train_scaled)  # You can apply to X_train, X_test, or the full dataset\n",
    "\n",
    "# Step 2: Calculate explained variance and cumulative variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 3: Plot the explained variance ratio and cumulative explained variance ratio\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the explained variance ratio for each component\n",
    "plt.plot(range(1, 21), explained_variance_ratio, 'o-', label='Explained Variance Ratio', color='b')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio and Cumulative Explained Variance')\n",
    "plt.xticks(range(1, 21))\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(range(1, 21), cumulative_explained_variance, 'o-', label='Cumulative Explained Variance', color='r')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Now reduce dimensionality to 2 components for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_train_scaled)  # The data is now reduced to 2 dimensions\n",
    "\n",
    "# Step 5: Plot the first two principal components\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Assuming y_train is the label and we have classes 0 to 3 (adjust labels/colors as per your data)\n",
    "plt.scatter(X_pca_2d[y_train == 0, 0], X_pca_2d[y_train == 0, 1], alpha=0.7, edgecolors='w', color='purple', s=50, label='Class 0')\n",
    "plt.scatter(X_pca_2d[y_train == 1, 0], X_pca_2d[y_train == 1, 1], alpha=0.7, edgecolors='w', color='yellow', s=50, label='Class 1')\n",
    "plt.scatter(X_pca_2d[y_train == 2, 0], X_pca_2d[y_train == 2, 1], alpha=0.7, edgecolors='w', color='black', s=50, label='Class 2')\n",
    "plt.scatter(X_pca_2d[y_train == 3, 0], X_pca_2d[y_train == 3, 1], alpha=0.7, edgecolors='w', color='blue', s=50, label='Class 3')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('First Two Principal Components of PCA')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Initialize the K-Means model\n",
    "kmeans_model = KMeans(n_clusters=len(set(y_train)), random_state=42)  # Adjust n_clusters based on your dataset\n",
    "\n",
    "# Fit the K-Means model using the training data\n",
    "kmeans_model.fit(X_train_scaled)\n",
    "\n",
    "# Predict cluster labels for training, test, and validation datasets\n",
    "train_clusters = kmeans_model.predict(X_train_scaled)\n",
    "test_clusters = kmeans_model.predict(X_test_scaled)\n",
    "validation_clusters = kmeans_model.predict(X_validation_scaled)\n",
    "\n",
    "# You can check inertia (within-cluster sum of squares) for evaluating K-Means\n",
    "print(f\"K-Means Inertia: {kmeans_model.inertia_:.4f}\")\n",
    "\n",
    "# Confusion matrix for K-Means (test set)\n",
    "conf_matrix_kmeans_test = confusion_matrix(y_test, test_clusters)\n",
    "print(\"\\nK-Means Test Confusion Matrix:\\n\", conf_matrix_kmeans_test)\n",
    "\n",
    "# Confusion matrix for K-Means (validation set)\n",
    "conf_matrix_kmeans_validation = confusion_matrix(y_validation, validation_clusters)\n",
    "print(\"\\nK-Means Validation Confusion Matrix:\\n\", conf_matrix_kmeans_validation)\n",
    "\n",
    "# Calculate the accuracy for K-Means clustering by comparing the clusters with true labels\n",
    "accuracy_kmeans_test = accuracy_score(y_test, test_clusters)\n",
    "accuracy_kmeans_validation = accuracy_score(y_validation, validation_clusters)\n",
    "\n",
    "print(f\"K-Means Test Accuracy: {accuracy_kmeans_test:.4f}\")\n",
    "print(f\"K-Means Validation Accuracy: {accuracy_kmeans_validation:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel='linear', C=0.1, random_state=42)  # You can adjust the kernel and C parameter as needed\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train2_scaled, y_train2)\n",
    "\n",
    "# Evaluate the SVM model on training, test, and validation datasets\n",
    "train_accuracy_svm = svm_model.score(X_train2_scaled, y_train2)\n",
    "test_accuracy_svm = svm_model.score(X_test_scaled, y_test)\n",
    "validation_accuracy_svm = svm_model.score(X_validation_scaled, y_validation)\n",
    "\n",
    "# Print training, test, and validation accuracy for SVM\n",
    "print(f\"SVM Training Accuracy: {train_accuracy_svm:.4f}\")\n",
    "print(f\"SVM Test Accuracy: {test_accuracy_svm:.4f}\")\n",
    "print(f\"SVM Validation Accuracy: {validation_accuracy_svm:.4f}\")\n",
    "\n",
    "# Generate confusion matrix and classification report for SVM on test and validation datasets\n",
    "test_predictions_svm = svm_model.predict(X_test_scaled)\n",
    "validation_predictions_svm = svm_model.predict(X_validation_scaled)\n",
    "\n",
    "conf_matrix_svm_test = confusion_matrix(y_test, test_predictions_svm)\n",
    "class_report_svm_test = classification_report(y_test, test_predictions_svm)\n",
    "print(\"\\nSVM Test Confusion Matrix:\\n\", conf_matrix_svm_test)\n",
    "print(\"\\nSVM Test Classification Report:\\n\", class_report_svm_test)\n",
    "\n",
    "conf_matrix_svm_validation = confusion_matrix(y_validation, validation_predictions_svm)\n",
    "class_report_svm_validation = classification_report(y_validation, validation_predictions_svm)\n",
    "print(\"\\nSVM Validation Confusion Matrix:\\n\", conf_matrix_svm_validation)\n",
    "print(\"\\nSVM Validation Classification Report:\\n\", class_report_svm_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Apply PCA to reduce the dataset to 10 components\n",
    "pca_10 = PCA(n_components=10)\n",
    "X_train_pca = pca_10.fit_transform(X_train_scaled)  # Reduce training data\n",
    "X_test_pca = pca_10.transform(X_test_scaled)        # Apply the same transformation to test data\n",
    "X_validation_pca = pca_10.transform(X_validation_scaled)  # Apply transformation to validation data\n",
    "\n",
    "# Step 2: Train the SVM model using the reduced dataset (10 PCA components)\n",
    "svm_model_pca = SVC(kernel='linear', C=1, random_state=42)\n",
    "svm_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 3: Train the K-Means Clustering model using the reduced dataset (10 PCA components)\n",
    "kmeans_model_pca = KMeans(n_clusters=len(set(y_train)), random_state=42)  # Number of clusters = number of classes\n",
    "kmeans_model_pca.fit(X_train_pca)\n",
    "\n",
    "# Step 4: Evaluate the SVM model\n",
    "# Accuracy on training, test, and validation sets\n",
    "train_accuracy_pca = svm_model_pca.score(X_train_pca, y_train)\n",
    "test_accuracy_pca = svm_model_pca.score(X_test_pca, y_test)\n",
    "validation_accuracy_pca = svm_model_pca.score(X_validation_pca, y_validation)\n",
    "\n",
    "print(f\"SVM with PCA (10 components) Training Accuracy: {train_accuracy_pca:.4f}\")\n",
    "print(f\"SVM with PCA (10 components) Test Accuracy: {test_accuracy_pca:.4f}\")\n",
    "print(f\"SVM with PCA (10 components) Validation Accuracy: {validation_accuracy_pca:.4f}\")\n",
    "\n",
    "# Step 5: Generate confusion matrix and classification report for the SVM model on test and validation sets\n",
    "test_predictions_pca = svm_model_pca.predict(X_test_pca)\n",
    "conf_matrix_pca_test = confusion_matrix(y_test, test_predictions_pca)\n",
    "class_report_pca_test = classification_report(y_test, test_predictions_pca)\n",
    "\n",
    "print(\"\\nSVM with PCA Test Confusion Matrix:\\n\", conf_matrix_pca_test)\n",
    "print(\"\\nSVM with PCA Test Classification Report:\\n\", class_report_pca_test)\n",
    "\n",
    "validation_predictions_pca = svm_model_pca.predict(X_validation_pca)\n",
    "conf_matrix_pca_validation = confusion_matrix(y_validation, validation_predictions_pca)\n",
    "class_report_pca_validation = classification_report(y_validation, validation_predictions_pca)\n",
    "\n",
    "print(\"\\nSVM with PCA Validation Confusion Matrix:\\n\", conf_matrix_pca_validation)\n",
    "print(\"\\nSVM with PCA Validation Classification Report:\\n\", class_report_pca_validation)\n",
    "\n",
    "# Step 6: Evaluate K-Means Clustering\n",
    "# Predict clusters for the test and validation datasets\n",
    "test_clusters_pca = kmeans_model_pca.predict(X_test_pca)\n",
    "validation_clusters_pca = kmeans_model_pca.predict(X_validation_pca)\n",
    "\n",
    "# Confusion matrix for K-Means (test set)\n",
    "conf_matrix_kmeans_test = confusion_matrix(y_test, test_clusters_pca)\n",
    "print(\"\\nK-Means with PCA Test Confusion Matrix:\\n\", conf_matrix_kmeans_test)\n",
    "\n",
    "# Confusion matrix for K-Means (validation set)\n",
    "conf_matrix_kmeans_validation = confusion_matrix(y_validation, validation_clusters_pca)\n",
    "print(\"\\nK-Means with PCA Validation Confusion Matrix:\\n\", conf_matrix_kmeans_validation)\n",
    "\n",
    "# Calculate the accuracy for K-Means clustering (by comparing clusters to labels)\n",
    "accuracy_kmeans_test = accuracy_score(y_test, test_clusters_pca)\n",
    "accuracy_kmeans_validation = accuracy_score(y_validation, validation_clusters_pca)\n",
    "\n",
    "print(f\"K-Means with PCA Test Accuracy: {accuracy_kmeans_test:.4f}\")\n",
    "print(f\"K-Means with PCA Validation Accuracy: {accuracy_kmeans_validation:.4f}\")\n",
    "\n",
    "# Step 7: Plot the explained variance ratio and cumulative variance (optional)\n",
    "explained_variance_ratio = pca_10.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), explained_variance_ratio, 'o-', label='Explained Variance Ratio', color='b')\n",
    "plt.plot(range(1, 11), cumulative_explained_variance, 'o-', label='Cumulative Explained Variance', color='r')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio and Cumulative Explained Variance (10 Components)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
